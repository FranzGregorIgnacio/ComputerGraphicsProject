Narrative for Category 1 and 2 Enhancements

The artifact that I had chosen for enhancements for Category One and Two was from a Computer Graphics course, CS 330. It was the final project in the class and I had chosen to render a Japanese shrine scene using C++ and OpenGL. Initially, the rendering pipeline for this project was static. Every object in the scene was initialized on its own and manually transformed, textured, and then rendered. This was a non-scalable setup that eventually became unmanageable if I had ever decided to expand the scene further. I had chosen this for Category One enhancement as I felt that it provided a good basis to show how I can improve and understand the structure and limitations of a graphics engine and a codebase. It also provides the foundation for me to be showcase how I can make code more modular and easier to use. To this end, I implemented a SceneNode class to encapsulate transformations, parent-child relationships, and rendering logic. This enhancement promotes not only modularity but reusability as well. Instead of hardcoding and manually managing every object and transformation, SceneNode handles this intrinsically and makes the overall code cleaner as a result. This enhancement has taught me some valuable lessons in regards to how I approach code. A major one is that separation of concerns and encapsulation are key to ensure that changes to parts of the code does not ripple and affect other possibly unrelated parts of the code. A key example was how I had actually hardcoded the boolean that dealt with allowing textures to be used into every render function prior to SceneNode. This was not only redundant to do multiple times but since I had also buried it in lines of code, I didn't realize that I had deleted all instances of it when I switched to the SceneNode render graph instead. This resulted in the project just opening to a black screen instead of displaying anything. Once this line of code had been reintegrated into the general pipeline again, the scene was displayed properly once more. I also learned how quickly the complexity of the code grows out of head without proper design and planning. Once I began doing the detailing such as transformations for the objects, I learned that a ton of the values had to be fine-tuned or experimented with to ensure the desired outcome. There were a few security flaws that would need to be ironed out if this project was expanded or provided with an endpoint of users to interact with. One major one is the lack of validation for file I/O operations for texture loading. These textures were hardcoded into every object's render before the implementation of SceneNode which could allow for malformed inputs or path traversal attacks to occur. Furthermore, sanitization of any inputs will ensure any attack vectors through these methods are stopped. 

I had also chosen this for Category Two enhancement because I felt that it provided a basis for different algorithms and data structures to create a complex feature such as a ray-casting and hit detection system. I implemented an Unprojection algorithm to convert from Screen Space coordinates to World Space coordinates so that the ray properly gets cast from the right location and in the right direction. The implementation of Unprojection showed me how critical accurate spatial mathematics is to any graphics engine. I had to understand how to go from Screen Space to Normalized Device Coordinates, and finally to World Space then back. The next algorithm used was Ray-Axis Aligned Bounding Box intersection where I checked whether the ray intersected with an object's bounding box. All of the objects actually created their own bounding box using a set of coordinates that were provided in the 3DShapes folder for the project. Using intersection tests and the parametric equation for a ray, I was able to test if the ray intersected with any of these bounding boxes as well as the distance to it. This algorithm leads into the next which is a Linear Search with Minimum Distance evaluation which allowed me to iterate through every object hit and highlight the one with the smallest distance to the camera which should be the first object the ray hits. I did this by storing hits in an array, specifically their distance, and then checking for the smallest distance. After that, I realized that I couldn't exactly tell if it was working correctly so I ended up outputting the memory address that the shape was stored at as well as turning them white to highlight them. There was only one major flaw that I never really figured out. Initially, the rays had been casting from world origin at 0,0 which in turn had resulted in unexplained behavior for the hits. After some fine-tuning, I was able to get it to cast from the camera instead. However, a persistent issue began where with enough distance, the highlighted object seems to be something to the left or right of the object with some instances of it being behind the object. None the less, the same security flaws from the first enhancement apply here as well. I believe that I was able to properly display four out of the five course outcomes since this project didn't necessarily require a security heavy mindset to move forward. As a result, I don't have any further updates to additional outcomes that my enhancements will cover. 
